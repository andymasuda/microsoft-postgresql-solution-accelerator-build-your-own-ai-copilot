# 4.1 Extract Data with Document Intelligence

 --TODO: Add brief intro to the below topics

## Building an AI enhanced Data Ingestion and Processing Pipeline

To build an AI-powered application that elevates data analysis with generative AI capabilities, we will utilize a comprehensive end-to-end solution pipeline. This pipeline begins with documents uploaded to Azure Blob Storage and utilizes Azure services for intelligent data ingestion, automated validation, semantic analysis, and optimized storage. By integrating AI-driven tools like Azure Document Intelligence and Azure AI services, the pipeline delivers reliable, accurate, and scalable data processing. Below is a detailed breakdown of how this pipeline is structured and enhanced.

---

### 1. Document Upload and Event-Driven Workflow

The pipeline begins with financial documents, such as statement of works, and invoices being uploaded to an **Azure Blob Storage** container. These uploads are performed via the existing application and written to existing blob storage, providing the starting point for the automated processing workflow.

- **Azure Blob Storage**: Cost-effective storage solution for incoming documents. It serves as the foundation for real-time and batch data ingestion.
- **Azure Event Grid**: Detects document upload events and triggers downstream processing steps, ensuring an event-driven architecture for real-time responsiveness and minimal latency.

This event-driven design ensures that documents are immediately processed upon arrival, eliminating delays and manual intervention. The event grid performs an HTTP POST request to a webhook hosted in the API which executes python code. The codelogic retrieves the document from Blob Storage and passes the document to **Azure Document Intelligence** (formerly Form Recognizer) for further processing.

---

### 2. AI-Enhanced Data Ingestion and Validation

The pipeline leverages **Azure Document Intelligence** and Azure AI services for automated extraction and validation. This step not only extracts text but also applies AI-driven validation to ensure the accuracy and reliability of the data.

- **Automated Text Extraction**: Azure Document Intelligence extracts structured and unstructured data from financial documents using pre-trained and custom models tailored for financial services.
  - Example: Extracting fields such as account numbers, transaction amounts, customer details, and regulatory clauses.

- **AI-Driven Data Validation**: Azure AI services validate the extracted data by cross-referencing it against predefined rules, business logic, and external datasets (e.g., regulatory guidelines or internal policies).
  - Example: Ensuring that financial transactions match expected formats and compliance requirements.
  - **Benefits**: Minimizes errors, increases accuracy, and ensures compliance with industry standards.

- **Semantic Chunking**: Large documents are broken into smaller, meaningful segments (semantic chunks) for downstream processing and efficient storage.

This AI-enhanced ingestion ensures that only high-quality, validated data progresses through the pipeline.

---

### 3. Intelligent Data Storage in Azure Database for PostgreSQL

The structured and validated text data is stored in an **Azure Database for PostgreSQL** instance. This database serves as the centralized repository for all processed financial documents, enabling efficient storage, querying, and integration with AI models.

- **Data Organization**:
  - Each document is stored as individual rows in the database, with columns for metadata (e.g., document ID, timestamp) and semantic chunks of text.
  
- **Scalability and Performance**: Azure Database for PostgreSQL is optimized for high-performance queries and supports workloads that involve large volumes of data, such as financial institutions managing millions of documents.

---

### 4. Generating Vector Embeddings with Azure AI Extension

To enable advanced AI functionalities, such as semantic search and ranking, the pipeline integrates the **Azure AI extension** for PostgreSQL. This extension allows embeddings to be generated directly within the database using **Azure OpenAI**.

- **Embedding Storage**:
  - A new column is added to the database to store high-dimensional vector embeddings of the document text:

    ```sql
    ALTER TABLE invoices ADD COLUMN embeddings VECTOR(1536);
    ALTER TABLE sows ADD COLUMN embeddings VECTOR(1536);
    ```

- **Embedding Generation**:
  - Azure OpenAI generates embeddings for the text chunks, enabling semantic understanding and efficient similarity searches. The process is seamless and happens directly in the database.

- **Applications**:
  - These embeddings enable capabilities like semantic search, ranking, and document clustering, enhancing downstream workflows for analytics and intelligent responses.

---

### Benefits of the Enhanced Pipeline

By incorporating **AI-driven validation** and seamless integration of **Azure Document Intelligence** and **Azure OpenAI**, this pipeline delivers:

- **High-Quality Data**: Ensures the reliability and accuracy of financial data through automated validation.
- **Scalability**: Handles large volumes of financial documents with real-time responsiveness.
- **Semantic Understanding**: Enables advanced AI capabilities like vector search and semantic ranking, paving the way for intelligent financial applications.
- **End-to-End Automation**: Reduces manual intervention, streamlines workflows, and ensures consistent processing.

This robust pipeline architecture transforms financial data into actionable insights, setting the stage for advanced analytics and intelligent copilot applications in the financial services industry.

## Build and train custom extraction models

TODO: https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/build-a-custom-model?view=doc-intel-4.0.0

To enhance the capabilities of the AI-powered application, building and training custom extraction models in Azure Document Intelligence allows for precise data extraction tailored to specific document types. This process leverages Azure’s powerful AI services to address domain-specific needs, such as extracting key fields from invoices, contracts, or financial statements, with high accuracy and efficiency. Below is an outline of how to build and train custom models for data extraction.

---

### **1. Define Use Case and Gather Training Data**

The first step in building a custom extraction model is to identify the specific use case and assemble a dataset of representative documents for training. This involves:

- **Use Case Definition**:
  - Clearly identify the fields or data points you want to extract (e.g., invoice numbers, account balances, customer names).
  - Ensure that the use case aligns with the documents' structure and content.

- **Training Data Collection**:
  - Gather a diverse set of sample documents (PDFs, scanned images, etc.) that reflect the variations in document structure.
  - Label the documents manually or use pre-existing annotations to mark the target data fields.

---

### **2. Upload Data to Azure Blob Storage**

Once the training dataset is prepared, upload the documents to **Azure Blob Storage**. Blob Storage serves as the repository for all training and testing documents required for building the custom model.

- **Organize the Data**:
  - Create folders to separate training and testing datasets.
  - Ensure proper naming conventions for easier management.

- **Secure the Storage**:
  - Implement appropriate security policies to protect sensitive financial data.

---

### **3. Train the Custom Extraction Model**

With the training data uploaded, use Azure Document Intelligence to train a custom model. Follow these steps:

1. **Access the Document Intelligence Studio**:
   - Navigate to the **Azure AI Document Intelligence Studio** in the Azure portal.
   - Select the option to create a new custom model.

2. **Label the Data**:
   - Use the built-in labeling tool to annotate fields in your documents.
   - For example, highlight and label fields like "Invoice Number," "Date," and "Total Amount."

3. **Train the Model**:
   - Once all documents are labeled, submit them for model training.
   - Specify the model type (e.g., template-based, neural model) based on the complexity of the documents.
   - Training will process the labeled data and generate a model capable of recognizing and extracting the specified fields.

4. **Test the Model**:
   - Validate the model by running it on the testing dataset.
   - Review the results to ensure the extracted fields are accurate and meet the use case requirements.

---

### **4. Publish and Integrate the Model**

After testing, publish the trained model to make it available for production use. The published model can then be integrated into your AI pipeline for automated data extraction.

- **Publishing**:
  - Assign a model ID and publish it within the Azure Document Intelligence environment.

- **Integration**:
  - Use the model’s endpoint in your workflow to process documents dynamically.
  - Combine it with other pipeline components, such as Azure Event Grid or Azure AI services, for seamless integration.

---

### **5. Monitor and Retrain**

Custom models require ongoing monitoring and retraining to maintain accuracy as document formats or content evolve.

- **Monitoring**:
  - Track performance metrics such as accuracy and confidence scores.
  - Identify documents where extraction failed or produced incorrect results.

- **Retraining**:
  - Collect new samples to retrain and fine-tune the model.
  - Repeat the training and testing process periodically to ensure model robustness.

---

### **Benefits of Custom Extraction Models**

- **Domain-Specific Accuracy**: Tailor data extraction to your organization’s unique document types.
- **Scalability**: Handle large volumes of documents with consistent performance.
- **Time Savings**: Automate repetitive data entry tasks, reducing manual effort.
- **Compliance**: Ensure accurate and reliable data extraction for regulatory and operational needs.

By leveraging Azure Document Intelligence to build and train custom extraction models, organizations can unlock the full potential of their AI pipelines, transforming unstructured documents into actionable insights with precision and efficiency.
