# 5.2 Implement the Chat Endpoint



Now all we need to do is run the FastAPI server, and have it listen for incoming requests from clients on these two API routes ("/" for health checks and "/api/create_response" for Contoso Chat). In the next section, we'll see how to do this locally for rapid prototyping and testing.


TODO: Add the following line as part of the description for the `chat` endpoint code...

6. is the entry point into our Contoso Chat implementation. It expects a customer ID, a question, and the chat history, and returns a text response.

7. **Define the copilot route** (line 51). The "/api/create_response" route maps to the endpoint where we can invoke the Contoso Chat implementation. 
    - It accepts POST requests from clients and extracts required parameters.
    - It invokes our copilot *get_request* function with those parameters.
    - It returns the copilot response to the client.

In this step, you will review backend API code for the `/chat` endpoint and the functions necessary for implementing a RAG pattern for including data from the PostgreSQL database in the composite prompt used by the LLM to generate completions.

TODO: Add steps for implementing the `/chat` endpoint in the API. This will be updated as they go, so start simple...

Have the users actually add the code (but libraries, such as LangChain should already be in place, added by the requirements.txt file)...

1. Implement `/chat` endpoint
   1. Code will already be there.
   2. Update to pull prompt from JSON file (ensure this is included in deployment)
   3. This should already have the langchain components in place

This endpoint should:

- Have RAG capablities (multi-agent or function calling...) using LangChain
- Cover usage of LangChain (already included code, so just cover the basics in descriptions)
- Show how to use LangChain's `StructuredTool` (or whatever it is) to call existing functions to get info from the database for RAG\

Next step is to refine the prompt to get better answers... (in next file)

## Review router...

## Add the completions router to the FastAPI app

TODO: Add steps to have them add the following line of code to `main.py` to add the `completions/chat` endpoint to the API.

```python
app.include_router(completions.router)
```

They can add it as either the first or last router in the list...


## Create LangChain agents

TODO: Add code for creating LangChain agents for performing data lookups

TODO: LangChain implementation should already be done, so this will just be reviewing the code a

How does this compare to using function calls for a single agent? Is it just splitting that across agents, so they perform data lookups in parallel?

Need to consider token limits...




---

!!! success "Congratulations! You have completed your setup and are ready to begin integrating AI into the solution."
